{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - A First Example\n",
    "\n",
    "After we've familiarized ourselves with the theory of machine learning, let's go into a practical example.\n",
    "\n",
    "The data available for download [here][data] contains measurements of the electrical energy output (EP) of a [combined cycle power plant][ccpp], together with a number of variables, containing\n",
    "\n",
    "- Ambient pressure (AP)\n",
    "- Relative humidity (RH)\n",
    "- Exhaust vacuum (V)\n",
    "- Temperature (T)\n",
    "\n",
    "We are interested in predicting the power output (PE).\n",
    "\n",
    "[data]: https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant\n",
    "[ccpp]: https://en.wikipedia.org/wiki/Combined_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print open('data/CCPP/Readme.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "The [pandas][pd] package is a powerful tool for loading small-ish datasets into Python, applying transformation, calculating aggregates and making basic visualizations. We'll learn more about `pandas` later in the course.\n",
    "\n",
    "[pd]: http://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make sure to chekc the other pandas.read_XXX functions\n",
    "data = pd.read_excel('data/CCPP/Folds5x2_pp.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the basic pandas object is the DataFrame\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get some basic information about the data columns,\n",
    "# very similar to R's summary function.\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance\n",
    "\n",
    "As we've learned in the theoretical part of the session, your target variable $Y$ *must* depend in some way on some of the inputs $X_i$ if are to have any hope of making a sensible prediction. One measure of this is the *covariance* of the two variables, denoted\n",
    "\n",
    "$$\\operatorname{Cov}(X_i, Y) =  \\operatorname{E}\\left[\\left(X_i - \\operatorname{E}[X_i]\\right)\\left(Y - \\operatorname{E}[Y]\\right)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculates the covariance between all pairs of variables\n",
    "# we want to _predict_ the power output PE, which means we\n",
    "# are interested in variables highly correlated or anticorrelated\n",
    "# with PE\n",
    "data.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a good correlation between `PE` and `V`, so let's choose `V` for our $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# enable inline plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "`pandas` supports a number of ways to make plots of your data. We'll discuss them in detail in a later session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot.scatter(x = 'V', y = 'PE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data\n",
    "\n",
    "Pandas supports two ways to access a given column `C` of a data frame `data`. The two syntaxes `data.C` and `data['C']` are mostly equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We've seen in the plot that we have multiple values for PE \n",
    "# for a given V. To make our lives easier, we take the mean\n",
    "# values for all columns for any given V. We will discuss \n",
    "# details of grouping and summarizing in a later session.\n",
    "import numpy as np\n",
    "# data.V == data['V'] # also possible\n",
    "data.V = np.round(data['V'], 1)\n",
    "data = data.groupby('V', as_index=False)\\\n",
    "    .mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot.scatter(x = 'V', y = 'PE')\n",
    "# looking much better already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Let's get to work and take a closer look at KNN.\n",
    "\n",
    "## Splitting in training and testing data.\n",
    "\n",
    "We want to fit a K-Nearest Neighbor model to our data, as discussed in the theory part. The objective will be to do as good as possible with predicting PE for unseen values of `V`. So how can we test our model? We choose to train it only on, say, 70% of the data available and use the 30% we left out as a stand-in for unseen data to test model performance. The function `train_test_split` in `sklearn.model_selection` does the splitting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[['V']],\n",
    "                                                    data.PE,\n",
    "                                                    test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "It's now time to train our K-Nearest Neighbor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose k=5 for starters\n",
    "five_nearest = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can now use the model to predict PE for a few values of V\n",
    "five_nearest.predict(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "five_nearest.predict(70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good did we do?\n",
    "\n",
    "Let's consider the residual sum of squares,\n",
    "\n",
    "$$\\operatorname{RSS}(k) = \\sum_i \\left(\\hat y_i^{(k)} - y_i\\right)^2$$\n",
    "\n",
    "where the sum runs over the indecs of testing data, and $\\hat y_i^{(k)}$ is the k-Nearest Neighbor prediction belonging to our $i$-th testing data point, while $y_k$ is the known value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum((five_nearest.predict(X_test) - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's repeat for k = 10\n",
    "ten_nearest = KNeighborsRegressor(n_neighbors = 10).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum((ten_nearest.predict(X_test) - y_test)**2)\n",
    "# we did a little better ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ... but tlet's be systematic here. We start \n",
    "# with defining a RSS function.\n",
    "def RSS(f, X, y):\n",
    "    return sum((f.predict(X) - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RSS(ten_nearest, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a scan over values for $k$ to better map out the dependence of RSS on $k$. What are sensible values? We start with 1 and stop when we have reached a $k$ of about half the size of the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ks = np.arange(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [KNeighborsRegressor(n_neighbors=k).fit(X_train, y_train)\n",
    "          for k in ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RSS_test = [RSS(f, X_test, y_test) for f in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(ks, RSS_test)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(\"RSS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance tradeoff\n",
    "\n",
    "For very small $k$, the models has high *variance*, i.e. the error is dominated by the noise in the data. If we choose $k$ too big, the model becomes *biased*, meaning that we don't reproduce the function's shape (i.e. that of `PE` in dependence of `V` very well). Somewhere in between lies the \"sweet spot\". A lot of work in machine learning projects goes into identifying that sweet spot.\n",
    "\n",
    "## Training data\n",
    "\n",
    "Let's now have a look at the RSS on our **training** data. What do we expect? We won't be able to do much on the high-$k$ side since our function just can't fit the underlying distribution well. But on the low-$k$ side it should have a decisive advantage: It *knows* the noise and should produce for $k$=1 a perfect fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RSS_train = [RSS(f, X_train, y_train) for f in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(ks, RSS_train)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(\"RSS_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare the two ...\n",
    "plt.plot(ks, RSS_test / RSS_test[-1], label = 'test')\n",
    "plt.plot(ks, RSS_train / RSS_train[-1], label = 'train')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(\"RSS, normalized\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degrees of freedom\n",
    "\n",
    "Another way of looking at this is analyzing the dependence of RSS on the degrees of freedom. Too few degrees of freedom don't allow us to reproduce the dependence of `PE` on `V` accurately. Too many degrees of freedm and we're \"chasing noise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(float(len(y_train))/ ks, RSS_test)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Effective DOF')\n",
    "plt.ylabel('RSS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional form\n",
    "\n",
    "Let's finally look at the functonal form of our k-Nearest Neighbor models. For a low $k$, we expect a high-noise, erratic behavior, while for high $k$, we expect a smoother and smoother function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ks = (1, 5, 30)\n",
    "plot_models = [models[list(ks).index(k)] for k in plot_ks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, f in zip(plot_ks, plot_models):\n",
    "    xs = [[i] for i in np.arange(65, 70, 0.1)]\n",
    "    plt.plot(xs, f.predict(xs), label=\"k = {}\".format(k))\n",
    "plt.plot(X_train, y_train, 'o', label='train')\n",
    "plt.plot(X_test, y_test, 's', label='test')\n",
    "plt.xlim(65, 70)\n",
    "plt.ylim(430, 455)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
